{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+k4MjHGVtiAQ11vDoMSUE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushantchandelog/Projects/blob/main/Philosophy_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "p95IQ1T_5z2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfLaSvn54fIt"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TextDataset\n",
        "  )\n",
        "from google.colab import drive\n",
        "from transformers import pipeline\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#google mount drive\n",
        "drive.mount('/content/drive', force_remount = True)  #it help refreshing the connection from google file and make sure we are wrking on a most current file\n",
        "\n",
        "#defing the path\n",
        "folder_path = \"/content/drive/MyDrive/cleaned_data\"\n",
        "COMBINED_FILE_PATH = f\"{folder_path}/combined_plato.txt\"\n",
        "OUTPUT_DIR = f\"{folder_path}/PhilosophyModel\"\n",
        "MODEL_NAME = \"gpt2\"\n",
        "\n",
        "print(\"data folder\", folder_path)\n",
        "print(\"Combined file Will be\", COMBINED_FILE_PATH)\n",
        "print(\"model will be saved to\", OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "G26FLKLm67ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combining the all nine files\n",
        "all_files = os.listdir(folder_path)\n",
        "txt_files =  [f for f in all_files if f.endswith('.txt') and f != \"combined_plato.txt\"]\n",
        "\n",
        "print(len(txt_files), \"files to combine:\", txt_files)"
      ],
      "metadata": {
        "id": "6dVokvar-toT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = \"\"\n",
        "for file_name in txt_files:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    with open(file_path, 'r', encoding='latin-1') as f:\n",
        "        all_text += f.read()\n",
        "\n",
        "    all_text += \"\\n\\n\" # Add separation between books\n",
        "\n",
        "\n",
        "#write the combin text for the new file\n",
        "with open(COMBINED_FILE_PATH, 'w', encoding='utf-8') as f:\n",
        "    f.write(all_text)\n",
        "\n",
        "print(\"Succesfully combine all file into \", COMBINED_FILE_PATH)"
      ],
      "metadata": {
        "id": "XKPnqi5vF38-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading tokenizer and base model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "print(\"tokenizer model loaded\")\n"
      ],
      "metadata": {
        "id": "2wrg6aCazfde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loding the combined datasets\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer = tokenizer,\n",
        "    file_path = COMBINED_FILE_PATH,\n",
        "    block_size= 128 #this is the chunk size for the text\n",
        ")\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer = tokenizer,\n",
        "    mlm = False\n",
        ")\n",
        "print(\"dataset is prepared\",len(train_dataset),\"text blocks\")"
      ],
      "metadata": {
        "id": "wbd1wZ7p0D-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up the trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,              # 3 passes over the data is a good start\n",
        "    per_device_train_batch_size=4,   # Batch size for T4 GPU\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = train_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "WETvy7iU1es7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#starting the training\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "\n",
        "#saving the final model\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "8up2jzcA74ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR  = \"/content/drive/MyDrive/cleaned_data/PhilosophyModel\"\n",
        "\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "TAcLPVm5EI4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the new model\n",
        "model_from_drive = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR)\n",
        "tokenizer_from_drive = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
        "plato_generator = pipeline(\n",
        "    'text-generation',\n",
        "    model=model_from_drive,\n",
        "    tokenizer=tokenizer_from_drive\n",
        ")\n"
      ],
      "metadata": {
        "id": "f5GHSsAo8c1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input(\"Enter you prompt for plato\")\n",
        "print(f\"Generating text for prompt: '{prompt}'\")\n",
        "generated_text = plato_generator(\n",
        "    prompt,\n",
        "    max_length=150,\n",
        "    num_return_sequences=1,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"\\n--- MODEL'S OUTPUT ---\")\n",
        "print(generated_text[0]['generated_text'])\n",
        "print(\"---------------------------------\")"
      ],
      "metadata": {
        "id": "Hazp1Vp3GMmL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}